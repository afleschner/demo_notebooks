{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qlsh5SvAxivM"
      },
      "source": [
        "You can follow along and play with this notebook by clicking the badge below\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/COGS118A/demo_notebooks/blob/main/lecture_04_linear_regression.ipynb)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtBwr1ytMk2N"
      },
      "source": [
        "## Linear regression\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M5NVgaq9Mk2R"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "\n",
        "\n",
        "# some utility functions to create toy data\n",
        "# data ~ underlying function + gaussian noise\n",
        "\n",
        "def create_toy_data(func, sample_size, std):\n",
        "    x = np.linspace(0, 1, sample_size).reshape(-1, 1)\n",
        "    t = func(x) + np.random.normal(scale=std, size=x.shape)\n",
        "    return x, t\n",
        "\n",
        "def a_sinusoidal_func(x):\n",
        "    return np.sin(2 * np.pi * x)\n",
        "\n",
        "def a_polynomial_func(x):\n",
        "    return (12. + 6.14*x - 8.4*x*x)\n",
        "\n",
        "def an_exp_func(x):\n",
        "    return (1+1*np.exp(0.001*x))\n",
        "\n",
        "def a_linear_func(x):\n",
        "    return (1.17 + 3.14*x)\n",
        "\n",
        "def a_discontinuous_func(x):\n",
        "    return [ 1. if el>0.5 else 0. for el in x  ]\n",
        "    \n",
        "sample_size = 10\n",
        "sigma = 0.3\n",
        "\n",
        "#func = a_sinusoidal_func\n",
        "#func = a_polynomial_func\n",
        "#func = a_discontinuous_func\n",
        "func = a_linear_func"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TAZJBgd1Mk2Y"
      },
      "source": [
        "# Some toy data generated by a linear function + noise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n85bRjlWMk2Y"
      },
      "outputs": [],
      "source": [
        "np.random.seed(1234)\n",
        "x_train, y_train = create_toy_data(func, sample_size, sigma)\n",
        "x_predict = np.linspace(0, 1, 100).reshape(-1, 1) #  some x-vals so we can generate true y-vals\n",
        "y_true = func(x_predict)\n",
        "\n",
        "\n",
        "plt.scatter(x_train, y_train, facecolor=\"none\", edgecolor=\"b\", s=50, label=\"training data\")\n",
        "plt.plot(x_predict, y_true, c=\"g\", label=\"generating function\")\n",
        "plt.ylim(bottom=0.0)\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oykPhEeNMk2d"
      },
      "source": [
        "# Polynomial features:   $ 1 + x + x^2 \\ldots$ "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hYKKE5IhMk2e"
      },
      "outputs": [],
      "source": [
        "features = PolynomialFeatures(degree=1)\n",
        "features.fit(x_train)\n",
        "features.transform(x_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dcy8mCvhMk2h"
      },
      "outputs": [],
      "source": [
        "features2 = PolynomialFeatures(degree=1)\n",
        "features2.fit(x_train)\n",
        "features2.transform(x_train)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTIV_vsOMk2k"
      },
      "source": [
        "# Solving for $\\mathbf{w}$ using closed-form solution of OLS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tVFrOUxrMk2l"
      },
      "source": [
        "$\\mathbf{w^*}= \\left( \\mathbf{X}^\\mathrm{T} \\mathbf{X}\\right)^{-1} \\mathbf{X}^\\mathrm{T} \\mathbf{y} $"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9FOKzB9rMk2l"
      },
      "outputs": [],
      "source": [
        "features = PolynomialFeatures(degree=1)\n",
        "features.fit(x_train)\n",
        "X_train = features.transform(x_train)\n",
        "\n",
        "# this is the one step variation\n",
        "# X_train = features.fit_transform(x_train)\n",
        "\n",
        "X_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FbagtqHMMk2n"
      },
      "outputs": [],
      "source": [
        "# in this code block, use numpy and \n",
        "# the closed form OLS solution above to find w^*\n",
        "# then print out what the optimal parameters are\n",
        "# YOUR CODE HERE\n",
        "raise NotImplementedError "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yyXfPN26Mk2p"
      },
      "source": [
        "The true generating function was $1.17 + 3.14*x$.  So if you did it right you will be in the zone of not too bad, but not right on. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EukEgvzRMk2p"
      },
      "source": [
        "Let's slow down and do that again so we can see the linear algebra in action. In the coming code cells, take your solution for $\\mathbf{w^*}$ above and break it up into chunks as it goes left to right... print the resulting matrices so you can see how the shape of the math changes during the equation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z56sKM0RMk2q",
        "outputId": "0621d855-1c7d-45a4-a4e5-55c55485d619"
      },
      "source": [
        "First do $\\mathbf{X}^\\mathrm{T}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IT50_y2OMk2r"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "outputId": "0621d855-1c7d-45a4-a4e5-55c55485d619",
        "id": "NEMQPP7DxivZ"
      },
      "source": [
        "Now do $\\left( \\mathbf{X}^\\mathrm{T} \\mathbf{X} \\right)$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sEm6mqkzxiva"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "outputId": "0621d855-1c7d-45a4-a4e5-55c55485d619",
        "id": "NmQ0uRTLxiva"
      },
      "source": [
        "Now do $\\left( \\mathbf{X}^\\mathrm{T} \\mathbf{X} \\right)^{-1}  $"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iig3Kz48xiva"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "outputId": "0621d855-1c7d-45a4-a4e5-55c55485d619",
        "id": "Rgo-a8bhxivb"
      },
      "source": [
        "Now do $\\left( \\mathbf{X}^\\mathrm{T} \\mathbf{X} \\right)^{-1} \\mathbf{X}^\\mathrm{T} $"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p2VkFzBFxivb"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "outputId": "0621d855-1c7d-45a4-a4e5-55c55485d619",
        "id": "LyuHgJDfxivb"
      },
      "source": [
        "you already know what the final step looks like, a (2,1) vector $\\left( \\mathbf{X}^\\mathrm{T} \\mathbf{X} \\right)^{-1} \\mathbf{X}^\\mathrm{T} \\mathbf{y} $"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-IK6q0fnMk2x"
      },
      "source": [
        "# There has to be an easier way!\n",
        "There is! __scikit-learn__ provides a uniform interface to dozens of machine learning algorithms.  They code up the math, and make the computation of the math effecient/fast."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "qwra-bPnMk2y",
        "outputId": "7a790ee5-277e-4258-decb-d585a8810f26"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-a3e7b3bf9835>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# if we hadn't have already built the bias term into features,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# you need to call fit_intercept = True!!!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLinearRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfit_intercept\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# YOUR CODE HERE!!! figure out how to call model.fit()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'LinearRegression' is not defined"
          ]
        }
      ],
      "source": [
        "# if we hadn't have already built the bias term into features, \n",
        "# you need to call fit_intercept = True!!!\n",
        "model = LinearRegression(fit_intercept=False) \n",
        "\n",
        "# YOUR CODE HERE!!! figure out how to call model.fit()\n",
        "# docs are at \n",
        "# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression\n",
        "model.fit(X+train, y_train)\n",
        "\n",
        "# now we will print out w*!\n",
        "print(model.coef_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bS03uMEMk2z"
      },
      "source": [
        "# OK, 1 last thing\n",
        "What if we try to do higher order curve fitting to data which is fundamentally linear?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "fazejeGHMk2z",
        "outputId": "619b00e9-a2e8-491c-b6e6-18c9e3d62e85",
        "scrolled": false
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-3eb484fe1fc3>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# make a graph with 2x2 subplots\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msharex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msharey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0maxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# loop through fitting/plotting 0th, 1st, 3rd, and 9th order polynomials\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
          ]
        }
      ],
      "source": [
        "# make a graph with 2x2 subplots\n",
        "fig, axes = plt.subplots(2,2,sharex=True, sharey=True,figsize=(10, 8))\n",
        "axs = axes.flatten()\n",
        "\n",
        "# loop through fitting/plotting 0th, 1st, 3rd, and 9th order polynomials\n",
        "for i, degree in enumerate([ 0, 1, 3, 9]):\n",
        "    ax = axs[i]\n",
        "    feature = PolynomialFeatures(degree)\n",
        "    X_train = feature.fit_transform(x_train)     # training dataset\n",
        "    X_predict = feature.fit_transform(x_predict) # validation dataset\n",
        "\n",
        "    model = LinearRegression(fit_intercept=False)\n",
        "\n",
        "    # YOUR CODE HERE!!! call model.fit() on training data\n",
        "    # docs are at \n",
        "    # https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "    # YOUR CODE HERE!!! call model.predict() on validation data\n",
        "    # docs are at \n",
        "    # https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression\n",
        "    y_predict = model.predict(X_predict)\n",
        "\n",
        "\n",
        "\n",
        "    ax.scatter(x_train, y_train, facecolor=\"none\", edgecolor=\"b\", s=50, label=\"training data\")\n",
        "    ax.plot(x_predict, y_true, c=\"g\", label=\"true function\")\n",
        "    ax.plot(x_predict, y_predict, c=\"r\", label=\"fitted curve\")\n",
        "    ax.annotate(\"M={}\".format(degree), xy=(.15, .05),  xycoords='axes fraction', fontsize=14)\n",
        "plt.legend(bbox_to_anchor=(1.05, 0.64), loc=2, borderaxespad=0.)\n",
        "plt.suptitle('Linear regression with 10 training data points',fontsize=20)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RlmoABuSxivc"
      },
      "source": [
        "Hey wait? Did we just use OLS (\"linear\") regression to fit a non-linear curve to data??? **Why did that work?**"
      ]
    }
  ],
  "metadata": {
    "anaconda-cloud": {},
    "colab": {
      "name": "Linear regression.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}